{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Data 620, Week 5, Part 2 Assignment\n",
    " #### Team 4: John Grando, Nick Capofari, Ken Markus, Armenoush Aslanian-Persico, Andrew Goldberg\n",
    " \n",
    " #### Project Details: \n",
    "Use a dataset to predict a class of new documents (either withheld from the training dataset or from another source such as your own spam folder). \n",
    "\n",
    "For this project, we used a pre-processed Enron e-mail corpus (available here: http://www2.aueb.gr/users/ion/data/enron-spam/) to classify the documents as either spam or ham. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Import and normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from IPython.display import display\n",
    "\n",
    "spamfolder = '/Users/andrew/Documents/School/Web Analytics/HW4/enron1/spam'\n",
    "spamdata = []\n",
    "for filename in os.listdir(spamfolder):\n",
    "    with open(spamfolder+'/'+filename) as spamtext:\n",
    "        spamtext = spamtext.read()\n",
    "        spamtext = spamtext.decode('UTF8', errors='ignore')\n",
    "        spamdata.append(spamtext)\n",
    "        \n",
    "hamfolder = '/Users/andrew/Documents/School/Web Analytics/HW4/enron1/ham'\n",
    "hamdata = []\n",
    "for filename in os.listdir(hamfolder):\n",
    "    with open(hamfolder+'/'+filename) as hamtext:\n",
    "        hamtext = hamtext.read()\n",
    "        hamtext = hamtext.decode('UTF8', errors='ignore')\n",
    "        hamdata.append(hamtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Subject: vastar resources , inc .\\r\\ngary , production from the high island larger block a - 1 # 2 commenced on\\r\\nsaturday at 2 : 00 p . m . at about 6 , 500 gross . carlos expects between 9 , 500 and\\r\\n10 , 000 gross for tomorrow . vastar owns 68 % of the gross production .\\r\\ngeorge x 3 - 6992\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by george weissman / hou / ect on 12 / 13 / 99 10 : 16\\r\\nam - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\ndaren j farmer\\r\\n12 / 10 / 99 10 : 38 am\\r\\nto : carlos j rodriguez / hou / ect @ ect\\r\\ncc : george weissman / hou / ect @ ect , melissa graves / hou / ect @ ect\\r\\nsubject : vastar resources , inc .\\r\\ncarlos ,\\r\\nplease call linda and get everything set up .\\r\\ni \\' m going to estimate 4 , 500 coming up tomorrow , with a 2 , 000 increase each\\r\\nfollowing day based on my conversations with bill fischer at bmar .\\r\\nd .\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by daren j farmer / hou / ect on 12 / 10 / 99 10 : 34\\r\\nam - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\nenron north america corp .\\r\\nfrom : george weissman 12 / 10 / 99 10 : 00 am\\r\\nto : daren j farmer / hou / ect @ ect\\r\\ncc : gary bryan / hou / ect @ ect , melissa graves / hou / ect @ ect\\r\\nsubject : vastar resources , inc .\\r\\ndarren ,\\r\\nthe attached appears to be a nomination from vastar resources , inc . for the\\r\\nhigh island larger block a - 1 # 2 ( previously , erroneously referred to as the\\r\\n# 1 well ) . vastar now expects the well to commence production sometime\\r\\ntomorrow . i told linda harris that we \\' d get her a telephone number in gas\\r\\ncontrol so she can provide notification of the turn - on tomorrow . linda \\' s\\r\\nnumbers , for the record , are 281 . 584 . 3359 voice and 713 . 312 . 1689 fax .\\r\\nwould you please see that someone contacts linda and advises her how to\\r\\nsubmit future nominations via e - mail , fax or voice ? thanks .\\r\\ngeorge x 3 - 6992\\r\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by george weissman / hou / ect on 12 / 10 / 99 09 : 44\\r\\nam - - - - - - - - - - - - - - - - - - - - - - - - - - -\\r\\n\" linda harris \" on 12 / 10 / 99 09 : 38 : 43 am\\r\\nto : george weissman / hou / ect @ ect\\r\\ncc :\\r\\nsubject : hi a - 1 # 2\\r\\neffective 12 - 11 - 99\\r\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\r\\n| | | |\\r\\n| mscf / d | min ftp | time |\\r\\n| | | |\\r\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\r\\n| | | |\\r\\n| 4 , 500 | 9 , 925 | 24 hours |\\r\\n| | | |\\r\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\r\\n| | | |\\r\\n| 6 , 000 | 9 , 908 | 24 hours |\\r\\n| | | |\\r\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\r\\n| | | |\\r\\n| 8 , 000 | 9 , 878 | 24 hours |\\r\\n| | | |\\r\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\r\\n| | | |\\r\\n| 10 , 000 | 9 , 840 | 24 hours |\\r\\n| | | |\\r\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\r\\n| | | |\\r\\n| 12 , 000 | 9 , 793 | 24 hours |\\r\\n| | | |\\r\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\r\\n| | | |\\r\\n| 14 , 000 | 9 , 738 | 24 hours |\\r\\n| | | |\\r\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\r\\n| | | |\\r\\n| 16 , 000 | 9 , 674 | 24 hours |\\r\\n| | | |\\r\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\r\\n| | | |\\r\\n| 18 , 000 | 9 , 602 | 24 hours |\\r\\n| | | |\\r\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\r\\n| | | |\\r\\n| 20 , 000 | 9 , 521 | 24 hours |\\r\\n| | | |\\r\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\r\\n| | | |\\r\\n| 22 , 000 | 9 , 431 | 24 hours |\\r\\n| | | |\\r\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\r\\n| | | |\\r\\n| 24 , 000 | 9 , 332 | 24 hours |\\r\\n| | | |\\r\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\r\\n| | | |\\r\\n| 26 , 000 | 9 , 224 | 24 hours |\\r\\n| | | |\\r\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\r\\n| | | |\\r\\n| 28 , 000 | 9 , 108 | 24 hours |\\r\\n| | | |\\r\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\r\\n| | | |\\r\\n| 30 , 000 | 8 , 982 | 24 hours |\\r\\n| | | |\\r\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\r\\n| | | |\\r\\n| 32 , 000 | 8 , 847 | 24 hours |\\r\\n| | | |\\r\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\r\\n| | | |\\r\\n| 34 , 000 | 8 , 703 | 24 hours |\\r\\n| | | |\\r\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\r\\n| | | |\\r\\n| 36 , 000 | 8 , 549 | 24 hours |\\r\\n| | | |\\r\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sample e-mail data\n",
    "hamdata[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Format and label e-mails spam/ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labeled_emails = ([(ham_mail.split(), 'ham') for ham_mail in hamdata] +\n",
    "                  [(spam_mail.split(), 'spam') for spam_mail in spamdata])\n",
    "import random\n",
    "random.seed(222)\n",
    "random.shuffle(labeled_emails)\n",
    "\n",
    "all_emails = [email for email, classification in labeled_emails[:500]]\n",
    "flattened_emails_all = [word for email in all_emails for word in email]\n",
    "\n",
    "all_emails_spam = [email for email, classification in labeled_emails[:500] if classification==\"spam\"]\n",
    "flattened_emails_spam = [word for email in all_emails_spam for word in email]\n",
    "\n",
    "all_emails_ham = [email for email, classification in labeled_emails[:500] if classification==\"ham\"]\n",
    "flattened_emails_ham = [word for email in all_emails_ham for word in email]\n",
    "\n",
    "tokenized_emails_all = []\n",
    "for word in flattened_emails_all:\n",
    "        tokenized_emails_all.extend(nltk.word_tokenize(word))\n",
    "tokenized_emails_spam = []\n",
    "for word in flattened_emails_spam:\n",
    "        tokenized_emails_spam.extend(nltk.word_tokenize(word))\n",
    "tokenized_emails_ham = []\n",
    "for word in flattened_emails_ham:\n",
    "        tokenized_emails_ham.extend(nltk.word_tokenize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Define feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "#extract the 500 most common words from each type and then from total\n",
    "word_freq_all = nltk.FreqDist(tokenized_emails_all)\n",
    "top_words_all = [w for (w,c) in word_freq_all.most_common(500)]\n",
    "top_words_all = [w for w in top_words_all if w.isalpha()]\n",
    "top_words_all = [w for w in top_words_all if w not in stopwords.words('english')]\n",
    "\n",
    "word_freq_ham = nltk.FreqDist(tokenized_emails_ham)\n",
    "top_words_ham = [w for (w,c) in word_freq_ham.most_common(250)]\n",
    "top_words_ham = [w for w in top_words_ham if w.isalpha()]\n",
    "top_words_ham = [w for w in top_words_ham if w not in stopwords.words('english')]\n",
    "\n",
    "word_freq_spam = nltk.FreqDist(tokenized_emails_spam)\n",
    "top_words_spam = [w for (w,c) in word_freq_spam.most_common(250)]\n",
    "top_words_spam = [w for w in top_words_spam if w.isalpha()]\n",
    "top_words_spam = [w for w in top_words_spam if w not in stopwords.words('english')]\n",
    "\n",
    "top_words = top_words_all + top_words_spam + top_words_ham\n",
    "top_words = list(set(top_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'-', 7721),\n",
       " (u'.', 5159),\n",
       " (u',', 3787),\n",
       " (u'/', 3321),\n",
       " (u':', 2745),\n",
       " (u'the', 2583),\n",
       " (u'to', 2025),\n",
       " (u'and', 1366),\n",
       " (u'ect', 1120),\n",
       " (u'of', 1051),\n",
       " (u'a', 1041),\n",
       " (u'for', 1037),\n",
       " (u'?', 931),\n",
       " (u'@', 922),\n",
       " (u'in', 816),\n",
       " (u'on', 791),\n",
       " (u'you', 780),\n",
       " (u'this', 735),\n",
       " (u'is', 700),\n",
       " (u'i', 641)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "#most common words\n",
    "print display(word_freq.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build feature extractor; uses both most common words and extremes in email length\n",
    "from __future__ import division\n",
    "import math\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    if len(document) < 20:\n",
    "        short_mail = True\n",
    "        long_mail = False\n",
    "    elif len(document) > 1500:\n",
    "        short_mail = False\n",
    "        long_mail = True\n",
    "    else:\n",
    "        short_mail = False\n",
    "        long_mail = False\n",
    "    features['len_check({})'.format(\"short_mail\")] = short_mail\n",
    "    features['len_check({})'.format(\"long_mail\")] = long_mail\n",
    "    word_length = 0\n",
    "    for word in top_words:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    for word in document:\n",
    "        word_length += len(word)\n",
    "    avg_word_length = word_length / len(document)\n",
    "    if avg_word_length > 3.9:\n",
    "        long_words = True\n",
    "        short_words = False\n",
    "    elif avg_word_length < 2.5:\n",
    "        long_words = False\n",
    "        short_words = True\n",
    "    else:\n",
    "        long_words = False\n",
    "        short_words = False\n",
    "    features['len_check({})'.format(\"long_words\")] = long_words\n",
    "    features['len_check({})'.format(\"short_words\")] = short_words\n",
    "    lex_div = len(document) / len(set(document))\n",
    "    if lex_div > 1.9:\n",
    "        features['len_check({})'.format(\"long_lex_div\")] = True\n",
    "    else:\n",
    "        features['len_check({})'.format(\"long_lex_div\")] = False\n",
    "    #features['len_check({})'.format(\"average_word\")] = round(word_length / len(document),1)\n",
    "    #features['lexical({})'.format(\"diversity\")] = round(len(document) / len(set(document)),1)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(document_features(d), c) for (d,c) in labeled_emails]\n",
    "train_set, dev_test_set, test_set = featuresets[:500], featuresets[500:1000], featuresets[1000:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "preds = pd.DataFrame({'spam or ham':[email for (email,classification) in dev_test_set],\n",
    "                      'observed':[classification for (email,classification) in dev_test_set],\n",
    "                      'predicted': [classifier.classify(document_features(n)) for (n,g) in labeled_emails[500:1000]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>predicted</th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>observed</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>316</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>0</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "predicted  ham  spam\n",
       "observed            \n",
       "ham        316    27\n",
       "spam         0   157"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(preds.observed,preds.predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impressive sensitivity at the expense of some specificity; some ham is predicted as spam. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Sensitivity : ', 1.0)\n",
      "('Specificity : ', 0.9212827988338192)\n"
     ]
    }
   ],
   "source": [
    "#Confusion matrix, Accuracy, sensitivity and specificity\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(preds.observed,preds.predicted)\n",
    "sensitivity1 = (float(cm[1,1])/(cm[1,1]+cm[1,0]))\n",
    "print('Sensitivity : ', sensitivity1 )\n",
    "\n",
    "specificity1 = (float(cm[0,0])/(cm[0,0]+cm[0,1]))\n",
    "print('Specificity : ', specificity1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Test Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95\n",
      "Most Informative Features\n",
      "            contains(cc) = True              ham : spam   =     30.6 : 1.0\n",
      "      contains(attached) = True              ham : spam   =     24.8 : 1.0\n",
      "    contains(securities) = True             spam : ham    =     21.9 : 1.0\n",
      "           contains(act) = True             spam : ham    =     21.9 : 1.0\n",
      "          contains(save) = True             spam : ham    =     21.9 : 1.0\n",
      "     contains(investing) = True             spam : ham    =     20.0 : 1.0\n",
      "           contains(ect) = True              ham : spam   =     19.0 : 1.0\n",
      "      contains(investor) = True             spam : ham    =     18.1 : 1.0\n",
      "     contains(investors) = True             spam : ham    =     18.1 : 1.0\n",
      "           contains(gas) = True              ham : spam   =     18.1 : 1.0\n",
      "          contains(easy) = True             spam : ham    =     17.7 : 1.0\n",
      "  contains(technologies) = True             spam : ham    =     14.3 : 1.0\n",
      "        contains(target) = True             spam : ham    =     14.3 : 1.0\n",
      "    contains(statements) = True             spam : ham    =     13.2 : 1.0\n",
      " contains(international) = True             spam : ham    =     12.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "print 'Accuracy: %4.2f' %nltk.classify.accuracy(classifier, dev_test_set)\n",
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "for (doc, tag) in labeled_emails[500:1000]:\n",
    "    guess = classifier.classify(document_features(doc))\n",
    "    accuracy = classifier.prob_classify(document_features(doc))\n",
    "    if guess != tag:\n",
    "        word_len = 0\n",
    "        for word in doc:\n",
    "            word_len += len(word)\n",
    "        errors.append( (tag, guess, len(doc), round(len(doc)/len(set(doc)),1), round(word_len/len(doc),2),\n",
    "                        accuracy.prob(\"spam\")) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>guess</th>\n",
       "      <th>length</th>\n",
       "      <th>lex div</th>\n",
       "      <th>avg word len</th>\n",
       "      <th>modeled spam probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>spam</td>\n",
       "      <td>112</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.08</td>\n",
       "      <td>0.985873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>spam</td>\n",
       "      <td>24</td>\n",
       "      <td>1.3</td>\n",
       "      <td>3.21</td>\n",
       "      <td>0.580058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>spam</td>\n",
       "      <td>27</td>\n",
       "      <td>1.1</td>\n",
       "      <td>4.04</td>\n",
       "      <td>0.996669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>spam</td>\n",
       "      <td>91</td>\n",
       "      <td>1.3</td>\n",
       "      <td>3.65</td>\n",
       "      <td>0.929502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>spam</td>\n",
       "      <td>11</td>\n",
       "      <td>1.1</td>\n",
       "      <td>2.82</td>\n",
       "      <td>0.967109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ham</td>\n",
       "      <td>spam</td>\n",
       "      <td>152</td>\n",
       "      <td>1.8</td>\n",
       "      <td>4.27</td>\n",
       "      <td>0.996847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>spam</td>\n",
       "      <td>13</td>\n",
       "      <td>1.1</td>\n",
       "      <td>4.85</td>\n",
       "      <td>0.991888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>spam</td>\n",
       "      <td>858</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3.89</td>\n",
       "      <td>0.999850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ham</td>\n",
       "      <td>spam</td>\n",
       "      <td>112</td>\n",
       "      <td>1.7</td>\n",
       "      <td>4.17</td>\n",
       "      <td>0.999996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ham</td>\n",
       "      <td>spam</td>\n",
       "      <td>434</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.79</td>\n",
       "      <td>0.999607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ham</td>\n",
       "      <td>spam</td>\n",
       "      <td>603</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.999156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ham</td>\n",
       "      <td>spam</td>\n",
       "      <td>397</td>\n",
       "      <td>2.4</td>\n",
       "      <td>4.46</td>\n",
       "      <td>0.999998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ham</td>\n",
       "      <td>spam</td>\n",
       "      <td>171</td>\n",
       "      <td>1.6</td>\n",
       "      <td>3.90</td>\n",
       "      <td>0.999994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ham</td>\n",
       "      <td>spam</td>\n",
       "      <td>14</td>\n",
       "      <td>1.3</td>\n",
       "      <td>3.14</td>\n",
       "      <td>0.660622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ham</td>\n",
       "      <td>spam</td>\n",
       "      <td>80</td>\n",
       "      <td>1.7</td>\n",
       "      <td>2.94</td>\n",
       "      <td>0.999996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ham</td>\n",
       "      <td>spam</td>\n",
       "      <td>12</td>\n",
       "      <td>1.1</td>\n",
       "      <td>4.33</td>\n",
       "      <td>0.634590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ham</td>\n",
       "      <td>spam</td>\n",
       "      <td>279</td>\n",
       "      <td>1.9</td>\n",
       "      <td>3.40</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ham</td>\n",
       "      <td>spam</td>\n",
       "      <td>2086</td>\n",
       "      <td>3.7</td>\n",
       "      <td>2.95</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ham</td>\n",
       "      <td>spam</td>\n",
       "      <td>545</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.85</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ham</td>\n",
       "      <td>spam</td>\n",
       "      <td>18</td>\n",
       "      <td>1.1</td>\n",
       "      <td>4.28</td>\n",
       "      <td>0.789881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ham</td>\n",
       "      <td>spam</td>\n",
       "      <td>30</td>\n",
       "      <td>1.3</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0.671105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ham</td>\n",
       "      <td>spam</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.50</td>\n",
       "      <td>0.995116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ham</td>\n",
       "      <td>spam</td>\n",
       "      <td>816</td>\n",
       "      <td>2.4</td>\n",
       "      <td>4.10</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ham</td>\n",
       "      <td>spam</td>\n",
       "      <td>98</td>\n",
       "      <td>1.8</td>\n",
       "      <td>3.88</td>\n",
       "      <td>0.894813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ham</td>\n",
       "      <td>spam</td>\n",
       "      <td>60</td>\n",
       "      <td>1.3</td>\n",
       "      <td>4.15</td>\n",
       "      <td>0.997683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ham</td>\n",
       "      <td>spam</td>\n",
       "      <td>1853</td>\n",
       "      <td>3.9</td>\n",
       "      <td>4.84</td>\n",
       "      <td>0.971820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ham</td>\n",
       "      <td>spam</td>\n",
       "      <td>66</td>\n",
       "      <td>1.4</td>\n",
       "      <td>4.20</td>\n",
       "      <td>0.729809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tag guess  length  lex div  avg word len  modeled spam probability\n",
       "0   ham  spam     112      1.5          3.08                  0.985873\n",
       "1   ham  spam      24      1.3          3.21                  0.580058\n",
       "2   ham  spam      27      1.1          4.04                  0.996669\n",
       "3   ham  spam      91      1.3          3.65                  0.929502\n",
       "4   ham  spam      11      1.1          2.82                  0.967109\n",
       "5   ham  spam     152      1.8          4.27                  0.996847\n",
       "6   ham  spam      13      1.1          4.85                  0.991888\n",
       "7   ham  spam     858      2.4          3.89                  0.999850\n",
       "8   ham  spam     112      1.7          4.17                  0.999996\n",
       "9   ham  spam     434      2.5          3.79                  0.999607\n",
       "10  ham  spam     603      3.0          3.33                  0.999156\n",
       "11  ham  spam     397      2.4          4.46                  0.999998\n",
       "12  ham  spam     171      1.6          3.90                  0.999994\n",
       "13  ham  spam      14      1.3          3.14                  0.660622\n",
       "14  ham  spam      80      1.7          2.94                  0.999996\n",
       "15  ham  spam      12      1.1          4.33                  0.634590\n",
       "16  ham  spam     279      1.9          3.40                  0.999999\n",
       "17  ham  spam    2086      3.7          2.95                  1.000000\n",
       "18  ham  spam     545      2.2          4.85                  1.000000\n",
       "19  ham  spam      18      1.1          4.28                  0.789881\n",
       "20  ham  spam      30      1.3          3.67                  0.671105\n",
       "21  ham  spam       6      1.0          6.50                  0.995116\n",
       "22  ham  spam     816      2.4          4.10                  0.999999\n",
       "23  ham  spam      98      1.8          3.88                  0.894813\n",
       "24  ham  spam      60      1.3          4.15                  0.997683\n",
       "25  ham  spam    1853      3.9          4.84                  0.971820\n",
       "26  ham  spam      66      1.4          4.20                  0.729809"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names = ['tag', 'guess', 'length', 'lex div', 'avg word len', 'modeled spam probability']\n",
    "pd.DataFrame(errors, columns = col_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>predicted</th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>observed</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>2670</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>7</td>\n",
       "      <td>1207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "predicted   ham  spam\n",
       "observed             \n",
       "ham        2670   288\n",
       "spam          7  1207"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf = pd.DataFrame({'spam or ham':[email for (email,classification) in test_set],\n",
    "                      'observed':[classification for (email,classification) in test_set],\n",
    "                      'predicted': [classifier.classify(document_features(n)) for (n,g) in labeled_emails[1000:]]})\n",
    "pd.crosstab(perf.observed,perf.predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that sensitivity remains very high, but some ham is unfortunately classified as spam. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Sensitivity : ', 0.9942339373970346)\n",
      "('Specificity : ', 0.9026369168356998)\n"
     ]
    }
   ],
   "source": [
    "#Confusion matrix, Accuracy, sensitivity and specificity\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(perf.observed,perf.predicted)\n",
    "sensitivity1 = (float(cm[1,1])/(cm[1,1]+cm[1,0]))\n",
    "print('Sensitivity : ', sensitivity1 )\n",
    "\n",
    "specificity1 = (float(cm[0,0])/(cm[0,0]+cm[0,1]))\n",
    "print('Specificity : ', specificity1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.93\n",
      "Most Informative Features\n",
      "            contains(cc) = True              ham : spam   =     30.6 : 1.0\n",
      "      contains(attached) = True              ham : spam   =     24.8 : 1.0\n",
      "    contains(securities) = True             spam : ham    =     21.9 : 1.0\n",
      "           contains(act) = True             spam : ham    =     21.9 : 1.0\n",
      "          contains(save) = True             spam : ham    =     21.9 : 1.0\n",
      "     contains(investing) = True             spam : ham    =     20.0 : 1.0\n",
      "           contains(ect) = True              ham : spam   =     19.0 : 1.0\n",
      "      contains(investor) = True             spam : ham    =     18.1 : 1.0\n",
      "     contains(investors) = True             spam : ham    =     18.1 : 1.0\n",
      "           contains(gas) = True              ham : spam   =     18.1 : 1.0\n",
      "          contains(easy) = True             spam : ham    =     17.7 : 1.0\n",
      "  contains(technologies) = True             spam : ham    =     14.3 : 1.0\n",
      "        contains(target) = True             spam : ham    =     14.3 : 1.0\n",
      "    contains(statements) = True             spam : ham    =     13.2 : 1.0\n",
      " contains(international) = True             spam : ham    =     12.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "print 'Accuracy: %4.2f' %nltk.classify.accuracy(classifier, test_set)\n",
    "classifier.show_most_informative_features(15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
